import argparse
import json

from transformers import AutoTokenizer
from utils import CustomDatasetForDev
from vllm_inference import CausalLMWithvLLM


def main(args):
    prompt_template = """You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.

Select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.
Do NOT provide any explanation for your choice.
Do NOT say both / neither are good.
You should answer using ONLY “Output (a)” or “Output (b)”. Do NOT output any other words.
Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. For example, the order in which the outputs were presented should NOT affect your judgment, as Output (a) and Output (b) are **equally likely** to be the better.

# Instruction:
{input}
# Output (a):
{output_1}
# Output (b):
{output_2}
# Which is better, Output (a) or Output (b)? Your response should be either “Output (a)” or “Output (b)”:"""
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    ds = CustomDatasetForDev(args, tokenizer, prompt_template)
    llm = CausalLMWithvLLM(
        model_path=args.model_path,
        use_chat_template=False,
        verbose=False,
        model_kwargs={
            "max_model_len": 4096,
            "tensor_parallel_size": 4,
            "gpu_memory_utilization": 0.9,
        },
        generation_config={
            "temperature": 0,
            "max_tokens": 4096,
            "repetition_penalty": 1.0,
        },
    )
    # 모델 설정 및 추론
    outputs = []
    preds = llm(ds.inp)
    outputs.extend(preds)
    try:
        with open(args.output_file, "w", encoding="utf-8") as f:
            f.write(json.dumps(outputs, ensure_ascii=False, indent=4))
        print(f"추론 결과가 {args.output_file}에 저장되었습니다.")
    except IOError as e:
        raise IOError(f"{args.output_file} 파일을 저장하는 중 오류가 발생했습니다: {str(e)}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Inference script for summarization model."
    )
    parser.add_argument(
        "--model_path", type=str, required=True, help="Path to the pre-trained model."
    )
    parser.add_argument(
        "--input_file", type=str, required=True, help="Path to the input JSON file."
    )
    parser.add_argument(
        "--output_file", type=str, required=True, help="Path to the output JSON file."
    )
    parser.add_argument(
        "--reverse",
        type=bool,
        required=False,
        default=False,
        help="Set Swap arg for testing",
    )
    args = parser.parse_args()

    main(args)
