defaults:
  - ppo: ppo
  - nn: nn
  - _self_

hydra:
  run:
    dir: outputs/${now:%Y-%m-%d/%H-%M-%S}

experiment:
  seed: 42
  device: 0
  gamma: 0.99
  finetuning_type: 'mix-to-mix'
  max_episode_steps: 1000 # the maximum number of episode steps ! Don't change this value
  num_rollout_steps: 128 # the number of policy rollout steps
  num_envs: 4 # envpool num_envs
  # async_batch_size: 32
  total_timesteps: 10000000 # 1000000 # 5000000 # null # 5000000 # 5M
  finetuning_timesteps: 100000 # 1000000
  save_ckpt: False
  num_checkpoints: 10
  capture_video: True
  cuda: True
  torch_deterministic: True
  resume: False
  resume_update_idx: 0
  resume_dir: None # output/2023-06-16/23-51-58/

  pretraining_env_ids: ['CartPole-v1',
  'InvertedPendulum-v4',
  # 'LunarLander-v2',
  # 'BipedalWalker-v3',
  # 'Acrobot-v1',
  # 'LunarLanderContinuous-v2',
  # 'Pendulum-v1',
  # 'MountainCarContinuous-v0',
  # 'BipedalWalkerHardcore-v3'
  ]
  finetuning_env_ids: [
  'Ant-v4',
  'Hopper-v4',
  'HalfCheetah-v4',
  'Reacher-v4',
  'InvertedDoublePendulum-v4',
  'Walker2d-v4',
  'Humanoid-v4',
  'Swimmer-v4',
  'Pusher-v4',
  'HumanoidStandup-v4']


evaluation:
  eval_seed: 42
  every: 5
  num_eval: 5 # the number of evaluations
  num_test_envs: 8 # gymnasium Syncvector num_envs

wandb:
  mode: online
  project: Mamba_MultiEnv_RL
  entity: null
  name: null
  group: null
  tags: null
  notes: null

paths:
  dir: outputs/${now:%Y-%m-%d/%H-%M-%S}
  log: outputs/${now:%Y-%m-%d/%H-%M-%S}/runs
  video: outputs/${now:%Y-%m-%d/%H-%M-%S}/videos
  checkpoints: outputs/${now:%Y-%m-%d/%H-%M-%S}/checkpoints
  src: outputs/${now:%Y-%m-%d/%H-%M-%S}/src
  scripts: outputs/${now:%Y-%m-%d/%H-%M-%S}/scripts